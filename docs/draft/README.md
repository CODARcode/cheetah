# CODAR Cheetah - The CODAR Experiment Harness

## Overview

* The CODAR Experiment Harness is designed to run Exascale science applications
  using different parameters and components to determine the best combination
  for deployment on different supercomputers.
* To use Cheetah, the user first writes a `campaign specification file` in python.
  In this file, one specifies:
     * which MPI programs to launch in parallel
     * what computing resources to give to those programs
     * which configuration files and binaries to copy to each experiment's directory
     * lists of possible values for some parameters (including computing resources) over which to do a grid search
     * how many times (iterations) to repeat each experiment to collect enough statistics
       to estimate the variability of the results.
* The campaign specification file does not define how programs communicate with each other.
  This is done in `adios2.xml` file that can be used to glue the input from one program
  with the output from another provided that all the involved programs are doing I/O using adios2.
  The data exchange can happen at each time step as the data is generated by producers.
  Adios2 shields a user from I/O details. Without recompiling a program, in `adios2.xml` file one
  can specify that output is written/read to/from a file or network socket, whether a producer should block
  for a certain number of reads by consumers or discard data after some timeout if consumers cannot keep up, etc.
* To generate shell scripts for all the specified experiments and a particular supercomputer,
  one executes  
  ```
  cheetah.py create-campaign -a <dir with configs & binaries> \
  	     -o <campaign dir> -e <specification>.py -m <supercomputer>
  ```  
* The experiments are organized into separate subdirectories that contain
  all the necessary executables and configuration files to run each experiment separately if necessary.
  Each such directory would also contain each own log files so that one can later examine what went
  wrong with a particular experiment.
* The top level directory of the campaign contains `run-all.sh` file that one can use to launch
  the whole set of experiments on the allocated resources.
* There is also `cancel.sh` that allows to stop the whole campaign.
* As campaign runs, one can examine its progress with
  ```
  cheetah.py status <campaign dir>
  ```
  or generate a detailed report with
  ```
  cheetah.py generate-report <campaign dir>
  ```
  Use `-h` option for a particular command to learn more details

## Cheetah installation
   * Dependency: Linux, python 3.5+
   * On supercomputers it should be installed on a parallel file system visible from compute/mother nodes
   * One can also do development on local computer
   * To install cheetah, do:
     ```
     git clone git@github.com:CODARcode/cheetah.git
     cd cheetah          
     python3 -m venv venv-cheetah
     source venv-cheetah/bin/activate
     pip install --editable .
     ```
   * Cheetah was tested so far on `summit`, `theta`, standalone Linux computers
   * It can also be installed with `spack`
## Setting up Cheetah environment
   ```
   cd <cheetah dir>
   source venv-cheetah/bin/activate
   ```
## Structure of the campaign file
   Here is a small example of a campaign file:
   ```
from codar.cheetah import Campaign
from codar.cheetah import parameters as p
from datetime import timedelta

class GrayScott(Campaign):
    name = "Gray-Scott-A"
    codes = [("gray-scott", dict(exe="gray-scott", sleep_after=1)), ("compression", dict(exe="compression")) ]
    supported_machines = ['local', 'theta']
    scheduler_options = {
        "theta": {
            "queue": "debug-flat-quad",
            "project": "XXXX",
        }
    }
    umask = '027'
    sweeps = [
     p.SweepGroup(name="Gray-Scott",
                  walltime=timedelta(minutes=30),
                  component_subdirs=True,
                  component_inputs={
                      'gray-scott': ['settings.json','adios2.xml'],
                      'compression': ['adios2.xml','sz.config','zc.config']
                  },
      parameter_groups=
      [p.Sweep([
          p.ParamCmdLineArg("gray-scott", "settings", 1, ["settings.json"]),
          p.ParamConfig("gray-scott", "L", "settings.json", "L",
                          [32, 64]),
          p.ParamConfig("gray-scott", "noise", "settings.json", "noise",
                          [0.01, 0.1]),
          p.ParamRunner('gray-scott', 'nprocs', [4] ),
          p.ParamCmdLineArg("compression", "input", 1, ["../gray-scott/gs.bp"]),
          p.ParamCmdLineArg("compression", "output", 2, ["compression.bp"]),
          p.ParamRunner('compression', 'nprocs', [1] )          
        ]),
      ]),
    ]

   ```
   * One needs to import `Campaign` and `parameters` modules.
   * To create your own campaign, you need to inherit from `Campaign` class and overwrite some of its fields:
     * `name` - campaign name
     * `codes` - what MPI programs to run in parallel.
       It is a dictionary mapping the campaign name of a program to the corresponding binary,
       possibly setting up some other parameters. In this example, `sleep_after=1` means that
       `gray-scott` started 1 second earlier than `compression` (is that right?? or the other way around??)
     * `supported_machines` - indicates for which supercomputer this campaign can be generated (why is it needed considering
       that it is defined when campaign is generated from the specification file??)
     * `scheduler_options` - defines some extra options to the scheduler not defined in `savanna`
       (used by `cheetah` to shield a user from the pecularities of a supercomputer), such as project
       to charge the run to
     * `umask` specifies the permissions for the newly created campaign files and directories.
     * `sweeps` is a list of `SweepGroups`
       * `SweepGroup` has a  name, a list of configuration files to copy into each experiment's directory,
       	 `parameter_groups`.
	 * `parameter_groups` is a list of `Sweeps` where one specifies with which parameters to run experiments.
	 * Some parameters are fixed values, and some are lists. A cartesian product of all the parameters are taken
	   to compute the experiments to perform.
     * Examples of parameter types:
       * `ParamCmdLineArg` allows to specify command line positional parameter for a particular program. For example
       	 ```
	 p.ParamCmdLineArg("gray-scott", "settings", 1, ["settings.json"])
	 ```
	 means that the first parameter of "gray-scott" program that in the campaign given a name "settings", has a value
	 "settings.json". Notice that the value is given as a list suggesting that you can list here all possible values
	 of the first positional parameter with which you want to experiment.	 
       * `ParamConfig` allows to deal with `*json` or `*ini` kind of parameter files. For example
	 ```
       	 p.ParamConfig("gray-scott", "L", "settings.json", "L", [32, 64])
	 ```
	 means that parameter "L" from "settings.json" (that "gray-scott" reads) can take values 32 and 64.
       * `ParamRunner` allows to specify resources for each program. For example
       	 ```
	 p.ParamRunner('gray-scott', 'nprocs', [4] )
	 ```
	 means that "gray-scott" would use 4 MPI ranks. As with any other cheetah parameters, one can specify several
	 values for such parameters as well which is needed for codesign studies.
       * Notice that parameters are given internal campaign name because one can use lambda functions to generate dependencies
       	 between different parameters and define "derived" parameters by using expressions with names of other parameters.
	 For example, ...
## Examples
