# CODAR Cheetah - The CODAR Experiment Harness

## Overview
* The CODAR Experiment Harness is designed to run Exascale science applications
  using different parameters and components to determine the best combination
  for deployment on different supercomputers.
* To use Cheetah, the user first writes a `campaign specification file` in python.
  In this file, one specifies:
  * which MPI programs to launch in parallel
  * what computing resources to give to those programs
  * which configuration files and binaries to copy to each experiment's directory
  * lists of possible values for some parameters (including computing resources) over which to do a grid search
  * how many times (iterations) to repeat each experiment to collect enough statistics
    to estimate the variability of the results.
* The campaign specification file does not define how programs communicate with each other.
  This is done in `adios2.xml` file that can be used to glue the input from one program
  with the output from another provided that all the involved programs are doing I/O using [ADIOS2](https://adios2.readthedocs.io/en/latest/index.html).
  The data exchange can happen at each time step as the data is generated by producers.
  ADIOS2 shields a user from I/O details. Without recompiling a program, in `adios2.xml` one
  can specify that output/input is written/read to/from a file or network socket, whether a producer should wait
  for a certain number of reads by consumers or discard data after some timeout if consumers cannot keep up, etc.
* To generate shell scripts for all the specified experiments and a particular supercomputer,
  one executes  
  ```
  cheetah.py create-campaign -a <dir with configs & binaries> \
  	     -o <campaign dir> -e <specification>.py -m <supercomputer>
  ```  
* The experiments are organized into separate subdirectories that contain
  all the necessary executables and configuration files to run each experiment separately if necessary.
  Each such directory would also contain each own log files so that one can later examine what went
  wrong with a particular experiment.
* The top level directory of the campaign contains `run-all.sh` file that one can use to launch
  the whole set of experiments on the allocated resources. Allocation is done by a user outside of cheetah using
  the resource manager of a particular supercomputer.
* There is also `cancel.sh` that allows to stop the whole campaign.
* As campaign runs, one can examine its progress with
  ```
  cheetah.py status <campaign dir>
  ```
  or generate a detailed report with
  ```
  cheetah.py generate-report <campaign dir>
  ```
  Use `-h` option for a particular command to learn more details

## Cheetah installation
* Dependency: Linux, python 3.5+
* On supercomputers it should be installed on a parallel file system visible from compute/mother nodes
* One can also do development on local computer
* To install cheetah, do:
  ```
  git clone git@github.com:CODARcode/cheetah.git
  cd cheetah          
  python3 -m venv venv-cheetah
  source venv-cheetah/bin/activate
  pip install --editable .
  ```
* Cheetah was tested so far on `Summit`, `Theta`, standalone Linux computers
* It can also be installed with `spack`

## Setting up Cheetah environment
   ```
   cd <cheetah dir>
   source venv-cheetah/bin/activate
   ```

## Campaign specification file
* Here is a small example of a campaign file:
  ```python
  from codar.cheetah import Campaign
  from codar.cheetah import parameters as p
  from datetime import timedelta

  class GrayScott(Campaign):
    name = "Gray-Scott-A"
    codes = [("gray-scott", dict(exe="gray-scott", sleep_after=1)),
             ("compression", dict(exe="compression")) ]
    supported_machines = ['local', 'theta']
    scheduler_options = {
        "theta": {
            "queue": "debug-flat-quad",
            "project": "XXXX",
        }
    }
    umask = '027'
    sweeps = [
     p.SweepGroup(name="Gray-Scott",
                  walltime=timedelta(minutes=30),
                  component_subdirs=True,
		  run_repetitions=2,
                  component_inputs={
                      'gray-scott': ['settings.json','adios2.xml'],
                      'compression': ['adios2.xml','sz.config','zc.config']
                  },
      parameter_groups=
      [p.Sweep([
          p.ParamCmdLineArg("gray-scott", "settings", 1, ["settings.json"]),
          p.ParamConfig("gray-scott", "L", "settings.json", "L",
                          [32, 64]),
          p.ParamConfig("gray-scott", "noise", "settings.json", "noise",
                          [0.01, 0.1]),
          p.ParamRunner('gray-scott', 'nprocs', [4] ),
          p.ParamCmdLineArg("compression", "input", 1, ["../gray-scott/gs.bp"]),
          p.ParamCmdLineArg("compression", "output", 2, ["compression.bp"]),
          p.ParamRunner('compression', 'nprocs', [1] )          
        ]),
      ]),
    ]
  ```
* One needs to import `Campaign` and `parameters` modules.
* To create your own campaign, you need to inherit from `Campaign` class and overwrite some of its fields:
  - `name` - campaign name
  - `codes` - what MPI programs to run in parallel.
    It is a dictionary mapping the campaign name of a program to the corresponding binary,
    possibly setting up some other parameters. In this example, `sleep_after=1` means that
    `gray-scott` started 1 second earlier than `compression` (is that right?? or the other way around??)
  - `supported_machines` - indicates for which supercomputer this campaign can be generated (why is it needed considering
    that it is defined when campaign is generated from the specification file??)
  - `scheduler_options` - defines some extra options to the scheduler not defined in `savanna`
    (used by `cheetah` to shield a user from the pecularities of a supercomputer), such as project
    to charge the run to
  - `umask` specifies the permissions for the newly created campaign files and directories.
  - `sweeps` is a list of `SweepGroups`
    + `SweepGroup` has a  name, a list of configuration files to copy into each experiment's directory,
      `parameter_groups`.
      * `parameter_groups` is a list of `Sweeps` where one specifies with which parameters to run experiments.
      * Some parameters are fixed values, and some are lists. A cartesian product of all the parameters are taken
	to compute the experiments to perform.
  - Examples of parameter types:
    + `ParamCmdLineArg` allows to specify command line positional parameter for a particular program.
      For example
      ```python
      p.ParamCmdLineArg("gray-scott", "settings", 1, ["settings.json"])
      ```
      means that the first parameter of "gray-scott" program that in the campaign given a name "settings", has a value
      "settings.json". Notice that the value is given as a list suggesting that you can list here all possible values
      of the first positional parameter with which you want to experiment.	 
    + `ParamConfig` allows to deal with `*json` or `*ini` kind of parameter files.
      For example
      ```python
      p.ParamConfig("gray-scott", "L", "settings.json", "L", [32, 64])
      ```
      means that parameter "L" from "settings.json" (that "gray-scott" reads) can take values 32 and 64.
    + `ParamRunner` allows to specify resources for each program.
      For example
      ```python
      p.ParamRunner('gray-scott', 'nprocs', [4] )
      ```
      means that "gray-scott" would use 4 MPI ranks. As with any other cheetah parameters, one can specify several
      values for such parameters as well which is needed for codesign studies.
    + When one creates a campaign with the above specification file, the campaign will have 4 experiments (2x2 parameter combinations).
    + Notice that parameters are given internal campaign name because one can use lambda functions to generate dependencies
      between different parameters and define "derived" parameters by using expressions with names of other parameters.
      For example, ...
    + To specify that each experiment should be repeated N times, one needs to set ``iterations``` field (??).
    + In each experiment specified above there are two MPI jobs running:
      - ["gray-scott"](https://github.com/pnorbert/adiosvm/tree/master/Tutorial/gray-scott) simulation  generates values on 3D grid at each time step, it uses 4 MPI ranks,
      - "compression" program at each time step
      	reads this 3D volume, compresses it with one of the compressors, such as [SZ](https://www.mcs.anl.gov/~shdi/download/sz-download.html),
	[ZFP](https://github.com/LLNL/zfp), [MGARD](https://github.com/CODARcode/MGARD.git), decompresses it back,
	runs [Z-Checker](https://github.com/CODARcode/Z-checker) and [FTK](https://github.com/CODARcode/ftk) to decide on the quality of
      	the compression, this job has 1 MPI rank.
  - Although ADIOS2 is not part of Cheetah, to understand how the programs, launched in parallel by Cheetah, communicate with each other,
    let us look into `adios2.xml` configuration file:
    ```
    <?xml version="1.0"?>
    <adios-config>
      <io name="SimulationOutput">
        <engine type="SST">
          <parameter key="RendezvousReaderCount" value="1"/>
          <parameter key="QueueLimit" value="15"/>
          <parameter key="QueueFullPolicy" value="Block"/>
        </engine>
      </io>
      <io name="CompressionOutput">
        <engine type="BPFile">
          <parameter key="RendezvousReaderCount" value="1"/>
          <parameter key="QueueLimit" value="15"/>
          <parameter key="QueueFullPolicy" value="Discard"/>
        </engine>
      </io>
    </adios-config>
    ```
    + Inside Gray-Scott program, using ADIOS2 API, a user opens "SimulationOutput" stream and writes to it at each time step
      without knowing what kind of I/O object it is: BP file, HDF5 file,
      network socket (SST, SSC), etc.
    + Inside compression program, using ADIOS2 API, a user  opens "SimulationOutput" stream and reads from it at each time step
    + The above XML file specifies that  "SimulationOutput" uses "SST" engine (network socket) and that a producer should block until somebody reads its output.
    + "CompressioOutput" stream is used by compression program to write its output into BP file (ADIOS2's native output format).
    + Engines can be changed in XML file without rebuilding the programs.

## Directory structure of the campaign
* Once one creates a campaign with the above specification file, the following directory structure is created:
  ```
  <campaign dir>/<username>/<campaign name>/run-<X>.iteration-<Y>
  ```
  - Here `campaign dir` is what is specified with `-o` option in `cheetah.py create-campaign -a . -o <campaign dir> ...`.
  - `campaign name` is what is set as `name` field in the specification file
  - `X` enumerates all possible combinations of parameters
  - `Y` goes over `run_repetitions` in SweepGroup
* Inside each `run-<X>.iteration-<Y>` there are subdirectories corresponding to the programs in the experiment. For example, for the above speficiation file, there
  are `gray-scott` and `compression` directories. There are also corresponding subdirectories with `codar.cheetah.tau-` prefix, which corresponds to the runs
  of the programs in which tau was used for profiling. Each subdirectory might contain configuration, launch, log files appropriate for the corresponding level.

## Examples
